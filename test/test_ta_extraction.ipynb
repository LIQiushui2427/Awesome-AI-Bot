{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('..')\n",
    "from data_feature_extraction.CoT_Dissa import extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = extract_data('../data/GC=F_com_disagg.csv', '../data/GC=F_com_disagg_finalextracted.csv')\n",
    "df = pd.read_csv('../data/GC=F_com_disagg_finalextracted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
       "       'Open_Interest_All', 'Swap_Positions_Long_All',\n",
       "       'Swap__Positions_Short_All', 'M_Money_Positions_Short_All',\n",
       "       'Other_Rept_Positions_Long_All', 'NonRept_Positions_Short_All',\n",
       "       'Pct_of_OI_Prod_Merc_Short_All', 'Pct_of_OI_Swap_Short_All',\n",
       "       'Pct_of_OI_Other_Rept_Short_All', 'Pct_of_OI_Tot_Rept_Short_All',\n",
       "       'Traders_Tot_All', 'Traders_M_Money_Long_All',\n",
       "       'Conc_Gross_LE_4_TDR_Long_All', 'Conc_Net_LE_4_TDR_Long_All',\n",
       "       'Conc_Net_LE_4_TDR_Short_All', 'volume_adi', 'volume_obv',\n",
       "       'volume_vwap', 'volume_nvi', 'volatility_atr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "df.set_index(['Date'])\n",
    "# Drop the string columns from the DataFrame\n",
    "df_ = df.drop(columns=string_cols, inplace=False)\n",
    "df_['Date'] = pd.to_datetime(df['Date'])\n",
    "df_.set_index(['Date'],drop=True, inplace=True)\n",
    "df_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lqs\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length):\n",
    "        self.df = df\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence_data = self.df.iloc[index: index + self.sequence_length]\n",
    "        sequence_target = self.df.iloc[index + self.sequence_length]\n",
    "        \n",
    "        # You will have to adjust the following lines based on the \n",
    "        # specific structure of your dataframe and the type of model you're building\n",
    "        x = torch.tensor(sequence_data.values).float()\n",
    "        y = torch.tensor(sequence_target.values).float()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class StockPriceTransformer(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers, dropout):\n",
    "        super(StockPriceTransformer, self).__init__()\n",
    "        self.input_linear = nn.Linear(feature_size, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=d_model*4, dropout=dropout), \n",
    "            num_layers)\n",
    "        self.output_linear = nn.Linear(d_model, 1)  # Predicting one value (next day's close price)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_linear(src)\n",
    "        src = src.transpose(0, 1)  # Transformer expects seq_len, batch, features\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.output_linear(output[-1])  # We only care about the last output for sequence-to-one.\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lqs\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([32, 26])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\lqs\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([31, 26])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 81724528298.6667, Validation Loss: 82903004501.3333\n",
      "Epoch: 2, Train Loss: 81721749845.3333, Validation Loss: 82899981653.3333\n",
      "Epoch: 3, Train Loss: 81718502741.3333, Validation Loss: 82896373077.3333\n",
      "Epoch: 4, Train Loss: 81714703360.0000, Validation Loss: 82892201984.0000\n",
      "Epoch: 5, Train Loss: 81710363648.0000, Validation Loss: 82887494314.6667\n",
      "Epoch: 6, Train Loss: 81705500672.0000, Validation Loss: 82882244608.0000\n",
      "Epoch: 7, Train Loss: 81700109312.0000, Validation Loss: 82876465152.0000\n",
      "Epoch: 8, Train Loss: 81694192298.6667, Validation Loss: 82870153216.0000\n",
      "Epoch: 9, Train Loss: 81687748266.6667, Validation Loss: 82863301973.3333\n",
      "Epoch: 10, Train Loss: 81680766293.3333, Validation Loss: 82855897770.6667\n",
      "Epoch: 11, Train Loss: 81673242965.3333, Validation Loss: 82847948800.0000\n",
      "Epoch: 12, Train Loss: 81665184426.6667, Validation Loss: 82839474176.0000\n",
      "Epoch: 13, Train Loss: 81656579072.0000, Validation Loss: 82830401536.0000\n",
      "Epoch: 14, Train Loss: 81647408810.6667, Validation Loss: 82820780032.0000\n",
      "Epoch: 15, Train Loss: 81637697877.3333, Validation Loss: 82810612394.6667\n",
      "Epoch: 16, Train Loss: 81627431936.0000, Validation Loss: 82799863125.3333\n",
      "Epoch: 17, Train Loss: 81616635221.3333, Validation Loss: 82788612778.6667\n",
      "Epoch: 18, Train Loss: 81605251754.6667, Validation Loss: 82776719360.0000\n",
      "Epoch: 19, Train Loss: 81593325226.6667, Validation Loss: 82764303018.6667\n",
      "Epoch: 20, Train Loss: 81580853589.3333, Validation Loss: 82751325525.3333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Defining the model hyperparameters\n",
    "d_model = 512  # The number of expected features in the input, it can vary based on your dataset\n",
    "nhead = 8  # The number of heads in the multiheadattention models, usually d_model is divisible by nhead\n",
    "num_layers = 3  # The number of sub-encoder-layers in the transformer encoder\n",
    "dropout = 0.1  # The dropout value\n",
    "feature_size = 26\n",
    "num_epochs = 20\n",
    "\n",
    "# dataloader configurations\n",
    "sequence_length = 50  # Define your sequence length\n",
    "batch_size = 32  # Define your batch size\n",
    "# Creating a dataset\n",
    "sequence_length = 50  # Define your sequence length\n",
    "dataset = TimeSeriesDataset(df_, sequence_length)\n",
    "\n",
    "# Splitting dataset into training set and testing set\n",
    "train_size = int(0.8 * len(dataset))  # 80% of the dataset\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Creating dataloaders\n",
    "batch_size = 32  # Define your batch size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = StockPriceTransformer(feature_size, d_model, nhead, num_layers, dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    valid_loss = test(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
